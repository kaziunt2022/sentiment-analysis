{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\burha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\burha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\burha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import re, string, nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#key = '5oXgUp9JOCQI4zE4gEennlnQpQQzEk2OqXdq6qCW'\n",
    "#key = \"ai8rfuwAsAyLP2KtxWjPPCKZsthRRbIRfveJKwjY\"\n",
    "key = \"NPIzRb1BwLXDivyCVjIdGBa6DCy0jrRB9d4AXWvj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(search_term):\n",
    "    # Get all dockit ids\n",
    "    print(\"Getting Dockits of\",search_term, \":....\")\n",
    "    url = \"https://api.regulations.gov/v4/dockets?filter[searchTerm]=\"+search_term+\"&api_key=\"+key\n",
    "    try: dockit_ids = [docit['id'] for docit in json.loads(requests.get(url).content)['data']]\n",
    "    except: \n",
    "        print(\"No dockit found!!!\")\n",
    "        return [], []\n",
    "    print(\"Successfully got Dockits of\",search_term, \":....\")\n",
    "    \n",
    "    # Get all documents from each dockit\n",
    "    print(\"Getting Document for each Dockit:....\")\n",
    "    document_object_ids = []\n",
    "    for dockit in dockit_ids:\n",
    "        url = \"https://api.regulations.gov/v4/documents?filter[docketId]=\"+dockit+\"&api_key=\"+key\n",
    "        for each_doc in json.loads(requests.get(url).content)['data']: document_object_ids.append(each_doc['attributes']['objectId'])\n",
    "    print(\"Successfully got all Document for each Dockit:....\")\n",
    "    \n",
    "    # Get Comment ids from \n",
    "    print(\"Getting Comment Ids for each document:....\")\n",
    "    comment_ids = []\n",
    "    for document in document_object_ids:\n",
    "        url = \"https://api.regulations.gov/v4/comments?filter[commentOnId]=\"+document+\"&api_key=\"+key\n",
    "        for each_doc in json.loads(requests.get(url).content)['data']: comment_ids.append(each_doc['id'])\n",
    "    print(\"Successfully got all Comment Ids for each document:....\")\n",
    "    \n",
    "    # Get comment from each Comment ID\n",
    "    print(\"Getting Comment for each Comment ID:....\")\n",
    "    comments=[]\n",
    "    for commentId in comment_ids:\n",
    "        url = \"https://api.regulations.gov/v4/comments/\"+commentId+\"?include=attachments&api_key=\"+key\n",
    "        try: comments.append(json.loads(requests.get(url).content)['data']['attributes']['comment'])\n",
    "        except: comments.append(\"\")\n",
    "    print(\"Got Comment for each Comment ID:....\")\n",
    "    \n",
    "    return comment_ids, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Getting Results for USA -----------------------\n",
      "Getting Dockits of USA :....\n",
      "Successfully got Dockits of USA :....\n",
      "Getting Document for each Dockit:....\n",
      "Successfully got all Document for each Dockit:....\n",
      "Getting Comment Ids for each document:....\n",
      "Successfully got all Comment Ids for each document:....\n",
      "Getting Comment for each Comment ID:....\n",
      "Got Comment for each Comment ID:....\n",
      "-------------------- Succcesseflly get Results for USA -----------------------\n",
      "\n",
      "-------------------- Getting Results for law -----------------------\n",
      "Getting Dockits of law :....\n",
      "Successfully got Dockits of law :....\n",
      "Getting Document for each Dockit:....\n",
      "Successfully got all Document for each Dockit:....\n",
      "Getting Comment Ids for each document:....\n",
      "Successfully got all Comment Ids for each document:....\n",
      "Getting Comment for each Comment ID:....\n",
      "Got Comment for each Comment ID:....\n",
      "-------------------- Succcesseflly get Results for law -----------------------\n",
      "\n",
      "-------------------- Getting Results for tax -----------------------\n",
      "Getting Dockits of tax :....\n",
      "Successfully got Dockits of tax :....\n",
      "Getting Document for each Dockit:....\n",
      "Successfully got all Document for each Dockit:....\n",
      "Getting Comment Ids for each document:....\n",
      "Successfully got all Comment Ids for each document:....\n",
      "Getting Comment for each Comment ID:....\n",
      "Got Comment for each Comment ID:....\n",
      "-------------------- Succcesseflly get Results for tax -----------------------\n",
      "\n",
      "-------------------- Getting Results for water -----------------------\n",
      "Getting Dockits of water :....\n",
      "Successfully got Dockits of water :....\n",
      "Getting Document for each Dockit:....\n",
      "Successfully got all Document for each Dockit:....\n",
      "Getting Comment Ids for each document:....\n",
      "Successfully got all Comment Ids for each document:....\n",
      "Getting Comment for each Comment ID:....\n",
      "Got Comment for each Comment ID:....\n",
      "-------------------- Succcesseflly get Results for water -----------------------\n",
      "\n",
      "-------------------- Getting Results for black -----------------------\n",
      "Getting Dockits of black :....\n",
      "Successfully got Dockits of black :....\n",
      "Getting Document for each Dockit:....\n",
      "Successfully got all Document for each Dockit:....\n",
      "Getting Comment Ids for each document:....\n",
      "Successfully got all Comment Ids for each document:....\n",
      "Getting Comment for each Comment ID:....\n",
      "Got Comment for each Comment ID:....\n",
      "-------------------- Succcesseflly get Results for black -----------------------\n",
      "\n",
      "-------------------- Getting Results for terrorism -----------------------\n",
      "Getting Dockits of terrorism :....\n",
      "No dockit found!!!\n",
      "-------------------- Succcesseflly get Results for terrorism -----------------------\n",
      "\n",
      "-------------------- Getting Results for foreign -----------------------\n",
      "Getting Dockits of foreign :....\n",
      "No dockit found!!!\n",
      "-------------------- Succcesseflly get Results for foreign -----------------------\n",
      "\n",
      "-------------------- Getting Results for nuclear -----------------------\n",
      "Getting Dockits of nuclear :....\n",
      "No dockit found!!!\n",
      "-------------------- Succcesseflly get Results for nuclear -----------------------\n",
      "\n",
      "-------------------- Getting Results for article -----------------------\n",
      "Getting Dockits of article :....\n",
      "No dockit found!!!\n",
      "-------------------- Succcesseflly get Results for article -----------------------\n",
      "\n",
      "-------------------- Getting Results for bank -----------------------\n",
      "Getting Dockits of bank :....\n",
      "No dockit found!!!\n",
      "-------------------- Succcesseflly get Results for bank -----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "searches = [\"USA\", \"law\", 'tax', \"water\", \"black\", \"terrorism\", \"foreign\", \"nuclear\", \"article\", \"bank\"]\n",
    "ids, comments = [], []\n",
    "for search in searches:\n",
    "    print(f\"-------------------- Getting Results for {search} -----------------------\")\n",
    "    result = get_comments(search)\n",
    "    ids+=result[0]\n",
    "    comments+=result[1]\n",
    "    print(f\"-------------------- Succcesseflly get Results for {search} -----------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Getting Results for terrorism -----------------------\n",
      "Getting Dockits of terrorism :....\n",
      "Successfully got Dockits of terrorism :....\n",
      "Getting Document for each Dockit:....\n",
      "Successfully got all Document for each Dockit:....\n",
      "Getting Comment Ids for each document:....\n",
      "Successfully got all Comment Ids for each document:....\n",
      "Getting Comment for each Comment ID:....\n",
      "Got Comment for each Comment ID:....\n",
      "*********** Got 146 Comments ************\n",
      "\n",
      "-------------------- Getting Results for foreign -----------------------\n",
      "Getting Dockits of foreign :....\n",
      "Successfully got Dockits of foreign :....\n",
      "Getting Document for each Dockit:....\n",
      "Successfully got all Document for each Dockit:....\n",
      "Getting Comment Ids for each document:....\n",
      "Successfully got all Comment Ids for each document:....\n",
      "Getting Comment for each Comment ID:....\n",
      "Got Comment for each Comment ID:....\n",
      "*********** Got 78 Comments ************\n",
      "\n",
      "-------------------- Getting Results for nuclear -----------------------\n",
      "Getting Dockits of nuclear :....\n",
      "Successfully got Dockits of nuclear :....\n",
      "Getting Document for each Dockit:....\n",
      "Successfully got all Document for each Dockit:....\n",
      "Getting Comment Ids for each document:....\n",
      "Successfully got all Comment Ids for each document:....\n",
      "Getting Comment for each Comment ID:....\n",
      "Got Comment for each Comment ID:....\n",
      "*********** Got 41 Comments ************\n",
      "\n",
      "-------------------- Getting Results for article -----------------------\n",
      "Getting Dockits of article :....\n",
      "Successfully got Dockits of article :....\n",
      "Getting Document for each Dockit:....\n",
      "Successfully got all Document for each Dockit:....\n",
      "Getting Comment Ids for each document:....\n",
      "Successfully got all Comment Ids for each document:....\n",
      "Getting Comment for each Comment ID:....\n",
      "Got Comment for each Comment ID:....\n",
      "*********** Got 47 Comments ************\n",
      "\n",
      "-------------------- Getting Results for bank -----------------------\n",
      "Getting Dockits of bank :....\n",
      "Successfully got Dockits of bank :....\n",
      "Getting Document for each Dockit:....\n",
      "Successfully got all Document for each Dockit:....\n",
      "Getting Comment Ids for each document:....\n",
      "Successfully got all Comment Ids for each document:....\n",
      "Getting Comment for each Comment ID:....\n",
      "Got Comment for each Comment ID:....\n",
      "*********** Got 230 Comments ************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "searches = [\"terrorism\", \"foreign\", \"nuclear\", \"article\", \"bank\"]\n",
    "for search in searches:\n",
    "    print(f\"-------------------- Getting Results for {search} -----------------------\")\n",
    "    result = get_comments(search)\n",
    "    ids+=result[0]\n",
    "    comments+=result[1]\n",
    "    total = len(result[1])\n",
    "    print(f\"*********** Got {total} Comments ************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.DataFrame(zip(ids, comments), columns=[\"CommectIds\", \"Comments\"])\n",
    "data_frame.to_csv(\"Comments.CSV\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english') # importing stopwords\n",
    "punctuations_list = string.punctuation # get punctuations\n",
    "lemmatizer = WordNetLemmatizer() # initialize word lemmatizer\n",
    "def preprocessing(text):\n",
    "    \"\"\"\n",
    "    This function will clean the givern text\n",
    "    \"\"\"\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())\n",
    "    text = text + \" \".join(emoticons).replace('-', '')\n",
    "    tokenize_text = [lemmatizer.lemmatize(word) for word in nltk.tokenize.word_tokenize(text) \n",
    "                     if (word not in stopwords_list) and (word not in punctuations_list) and (len(word)>=2) and (word.isalpha())]\n",
    "    return \" \".join(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Comments.CSV\")\n",
    "df.dropna(inplace=True, subset=[\"Comments\"])\n",
    "comments_df = df[\"Comments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = comments_df.apply(preprocessing)\n",
    "analyzier = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiments(text):\n",
    "    x= analyzier.polarity_scores(text)['compound']\n",
    "    if x>= 0.05: return \"Positive\"\n",
    "    elif x<= -0.05: return \"Negative\"\n",
    "    else: return \"Neutral\"\n",
    "\n",
    "def sentiments_score(text):\n",
    "    return analyzier.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Clean Comments\"] = comments_df\n",
    "df['Sentiment Scores'] = comments_df.apply(sentiments_score)\n",
    "df[\"Sentiments\"] = comments_df.apply(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Comments With Sentiments.CSV\", index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e49d90822828a9cccc11f2f5f5a36c8a22e5085827c840ea9f0df92798127a1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
